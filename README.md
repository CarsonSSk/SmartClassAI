## Project Overview

### PART 1: Data Collection, Cleaning, Labeling & Preliminary Analysis

In this part, we focus on:

- Collecting suitable training data
- Cleaning and labeling the dataset
- Visualizing the dataset to improve the cleaning methods and diagnose any issues with the dataset

### PART 2: Deep Learning Model Development and Evaluation

In this part, we focus on:

- Creating an AI capable of analyzing facial images for classification using PyTorch
- Design and train this Convolutional Neural Network on the classes as outlined in Part I
- Generate a thorough evaluation of our model

## Table of Contents

Part 1: Contains all folders and .py scripts.

- data: Folder that contains all data images and .csv files
  - dataSources: Folder that contains the original images split into their sources and classes.
    - fer2013: Images from the FER-2013 dataset.
      - Angry, Happy, Neutral folders containing images from their respective class. Empty Engaged folder (no images from this source in this class)
    - kdef: Images from the KDEF dataset.
      - Engaged folder containing images for this class. Angry, Happy, Neutral folders are empty (no images from this source in these classes.)
    - lfiw: Images from the Labeled Faces In the Wild dataset.
      - Engaged folder containing images for this class. Angry, Happy, Neutral folders are empty (no images from this source in these classes.)
    - teammates: Images taken by each team member.
      - Angry, Happy, Neutral and Engaged folders containing images from their respective class.
  - cleanedData: Folder that contains the images after data cleaning methods were applied, split into the 4 classes.
    - Angry, Happy, Neutral and Engaged folders containing images from their respective class.
  - additionalLabels: Contains the folders used for the labelling of the additional attributes: gender (female, male, other) and age group (young, middle aged, senior, other).
  - biasMitigationExtraBatch: Folder in which to place the images to add to the original dataset in order to mitigate bias. In our case, only images of old male and female individuals had to be added.
  - models: Directory that contains all models. Inside each of the subfolders which are named after the respective models, you will find: the model paramters file (.pth), CSV files containing the results of that model on the testing dataset, CSV containing the validation loss from the training process, CSV containing the accuracy of the model.
  - runFiles: Directory containing images that you wish to run a specific model on (single image + batch of images are possible) using run_model.py.
    - imagesDirectory: Directory where you place the images you want to apply the specific model on.
    - runCleanedImages: Directory that contains the cleaned version of the images you wish to predict. The cleaned images will be generated automatically based on the same methodology used to clean the training/testing image data.
    - directory_images_data.csv: CSV file containing the predictions for all the images in the batch that you last ran the run_model.py script on.
  - image_data.csv: CSV that stores the image data from the entire dataset from dataSources (Image Name, Path, Label, Source, CompositeName)
  - image_data_shuffled: CSV that stores the same data as image_data.csv, but in a shuffled order (can be reshuffled with dataset_segmentation.py).
  - cleaned_image_data.csv: CSV that stores the data of the CLEANED images from the entire dataset from cleanedData (Image Name, Path, Label, Source, CompositeName)
  - train_dataset.csv: CSV that stores the data of the images assigned to the training dataset (Path, CompositeName, Label, Image Name, Source)
  - validation_dataset.csv: CSV that stores the data of the images assigned to the validation dataset (Path, CompositeName, Label, Image Name, Source)
  - test_dataset.csv: CSV that stores the data of the images assigned to the testing dataset (Path, CompositeName, Label, Image Name, Source)
  - kfold_dataset.csv: CSV that stores all the data used for training and testing our final model in Part II. (holds 2000 images)
- data_cleaning.py: Script that cleans the original data according to cleaning techniques, which are easily modifiable.
- dataset_segmentation.py: Script that allows random selection of a specified number of images within the original data, for each class, and splits them into 3 datasets (training, validation, testing) to train, validate and test our classification algorithm. Number of images, proportion of images in each split, and randomization seed can be modified by the user. The script also creates a shuffled list of all images.
- model_evaluation.py: (For Part 2) Generates evaluation metrics for the specified models (main, variant 1, variant 2, best).
- model_valuation_summary.csv: Table with the evaluation metrics generated by model_evaluation.py.
- model_training.py: Trains 3 models with specified hyperparamers. The architecture of each model (main, variant 1, variant 2) as well was the various methods used such as early stopping are described in the report.
- run_model.csv: Run a specified model on a single image and a batch of images. Prints the predicted facial emotion in the output, and generates a CSV in runFiles for the batch of images. **Part 3: Added a section to also run the specified model on the test_dataset.csv, in order to generate a list of predictions vs. real labels within results_run_test.csv.
- results_run_test.csv: Contains the real labels and predicted labels generated from the last model run with run_model.py. Can then be used with bias_analysis to generate the bias analysis metrics.
- data_visualization.py: Script to visualize the data in many ways: plot the class distribution of images, plot the pixel intensity distributions within each class, sample 15 random images from each class with their corresponding pixel intensity distribution.
- bias_analysis_results.csv: CSV generated by bias_analysis.py.
- bias_analysis.py: Generate a bias analysis for gender and age based on the lastest model that was run with run_model.py (reads directly from results_run_test.csv and cross-links with combined_images_train_validation_test.csv/withBiasMitigation_combined_images_train_validation_test.csv to associate the evaluation metrics with the correct attribute labeling). Generates a CSV containing the metrics.
- combined_images_train_validation_test.csv: CSV containing all images from the original training, validation and test datasets, their paths, their label for emotion, age, gender, their corresponding dataset (training vs validation vs test).
- create_bias_mitigation_dataset.py: Script to create/update the with_biasMitigation_combined_images_train_validation_test.csv with any additional images that we wish to add to the original dataset in order to do bias mitigation (in our case, adding images of old individuals to mitigate the bias against older individuals in terms of accuracy).
- unbiased_model_evaluation.py: Duplicate of model_evaluation.py to use for the post-bias mitigation models.
- unbiased_model_training.py: Duplicate of model_training.py to use to generate the post-bias mitigation model. It is practically the same, except we have removed the variants for faster runtime, increased the patience to 5 for more consistent results between runs/different seeds, and the training is now done on the post-mitigation dataset (more training images of old individuals in our case).
- with_biasMitigation_combined_images_train_validation_test.csv: CSV containing all images from the original, as well as the post bias mitigation, training, validation and test datasets, their paths, their label for emotion, age, gender, their corresponding dataset (training vs validation vs test).

PART_3:
- Part2_kfold_models: holds the 10 models created at each fold when using the same dataset as in Part II.
  - models 1-10.
  - confusion_matrix.png: The resulting confusion matrix using the overall_test_results.csv.
  - overall_test_results.csv: Holds the predicted and true labels after testing the 10 models.
- Part3_kfold_models: holds the 10 models created at each fold when using the new dataset created for bias mitigation in Part III.
  - models 1-10.
- kfold_dataset.py: Creates the combined dataset with all the data we used for training and testing our models in Part II. Creates the kfold_dataset.csv.
- kfold_script.py: Same code as our model_training.py but only has the main model implemented and has the k-fold validation incorporated within the training loop.
- README.md: Read me file.

## Start up dependencies

### 1. Set Up the Environment Using `venv`

1. **Navigate to the project directory:**

   ```bash
   ex.: cd ~/Desktop/COMP472-Project/PART_1
   ```

2. **Create a virtual environment:**

   ```bash
   python -m venv venv
   ```

3. **Activate the virtual environment:**
   - **Windows Command Prompt:**
     ```bash
     venv\Scripts\activate
     ```
   - **Windows PowerShell:**
     ```bash
     .\venv\Scripts\Activate
     ```
   - **Git Bash or WSL:**
     ```bash
     sourceData venv/Scripts/activate
     ```
   - **Linux/macOS Terminal:**
     ```bash
     sourceData venv/bin/activate
     ```
   - **If `cannot be loaded because running scripts is disabled on this system`:**
     Try `Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser`
4. **Install the required packages:**
   ```bash
   pip install -r requirements.txt
   ```

## Steps to execute code

### Part 1

a) Data cleaning:

1. First, make sure PART_1/data/dataSources contains images in their respective folders. These will be the ones to be cleaned into the PART_1/data/cleanedData folder.
2. Open data_cleaning.py.
3. In the process_image function, locate the various cleaning methods implemented. Comment in/out any method that you wish to apply or not on the original images (Grayscaling, Resizing, Histogram equalization, CLAHE, Blurring).
4. Run data_cleaning.py.
5. Cleaned images will now be in the cleanedData folder, and cleaned_image_data.csv will contain the corresponding data.

b) Data visualization (with Matplotlib):

1. First, make sure the PART_1/data/cleanedData folder contains images (the script has been configured to visualize post-cleaning data).
2. Open data_visualization.py.
3. At the end of the script, comment in/out any execution of the plot_class_distribution, plot_sample_images, plot_pixel_intensity_distribution functions that you wish to run or not.
4. Run data_visualization.py. Plots/sample images will pop-up sequentially. In order to view the next plot/image sample, the current pop-up must be closed to allow the program to continue to run. Plots and sample images can be saved.

### Part 2 (you can find our final models and some of our sample models here: https://drive.google.com/drive/folders/1sjcfjRKqo8a35taRH3w1eqJgEFPJmiDV)
c) Model training (with PyTorch):

1. First, make sure the PART_1/data/cleanedData folder contains images (the script has been configured to train on the cleaned data).
2. Open model_training.py.
3. At the beginning of the script, establish the parameters for the training: num_epochs (minimum number of epochs, the maximum has been set to 30), learning rate (set to 0.005), patience parameter (how many epochs without decreasing validation loss before the training stops). Possibility to set a random seed if you want to obtain consistent models each run (same results). Set "reproducibility" to false if you want new results on each run of the script. Note: Reproducibility only works when run on the same machine, with the same Python version. Running on two different machines will guarantee DIFFERENT results.
4. Run model_training.py. Wait for the models to be trained (this step can take up to 15 minutes). Validation losses, epochs, accuracies and model names will be displayed in the output. At the end, there will be pop-ups of sample images from the testing dataset with predictions and their respective correct labels, for each of the three models trained (main, variant 1, variant 2). Each pop-up has to be closed for the next one to show up.
5. When the script is finished running, all 3 models will be saved in a unique directory in the models folder, as well as their respective prediction results, accuracies, and validation losses, all in csv format. The model is saved as well in .pth format.

d) Model evaluation (with Matplotlib and Sklearn)

1. First, make sure the PART_1/models contains the models you wish to evaluate, with their respective results.csv files (the script has been configured to generate the evaluations from the results csv which is generated when training the model, and then testing it on the testing dataset, all which is done when running model_training.py).
2. Open model_evaluation.py.
3. At the beginning, establish the model names of the models that you wish to evaluate. You can simply copy-paste the folder names from the models directory. The model folder must contain the results.csv of the model you wish to evaluate.
4. Run model_evaluation.py.
5. There will be pop-ups with the confusion matrices of each evaluated model, sequentially (close a pop-up to move on to the next).  Evaluation metrics for each model will be printed in the output, and saved in model_evaluation_summary.csv, located in the main directory.

e) Run/apply model on custom image / batch of images

1. First, open the "runFiles" directory in the main directory. Place the single image you wish to predict in this directory, and place the batch of images that you wish to predict in the "imagesDirectory" subdirectory. For the best results, the images must be: square format (will be resized to 48x48 so distortion may happen); can be RGB or grayscale; ideally, as much background should have been cropped out of the image manually in order to center the image on the face.
2. Open run_model.py.
3. At the beginning, set the paths to the images you wish to predict: image_path for the single image; directory_path for the batch of images. Set the path to the model you wish to use as well as the appropriate model type (0 = main model, 1 = variant 1, 2 = variant 2).
4. Run run_model.py.
5. In the output, you will see the prediction of the model for each image's facial emotion. You can check if it is right or wrong! The results for the batch of images will also be saved in a csv file: runFiles/directory_images_data.csv, which contains each of the batch's image name, path, and predicted label. This allows for further analysis of the predictions if desired by the user.

### Part 3

f) Run K-fold with Main model from Part II and with final model from Part III:

1. Run kfold_dataset.py to generate the combined dataset (kfold_dataset.csv) used for training and testing models in Part II.
2. Go in the kfold_script.py and set which model you want to perform k-fold on. This selection is done on line 40. Set the dataset_choice to 'PART_2' or 'PART_3'.
3. Set dataset_choice to 'PART_2' and run the script.
4. Set dataset_choice to 'PART_3' and run the script again. 
5. The results for each fold, including performance metrics and confusion matrices, will be saved and can be used for further analysis and reporting. Check PART_3\Part2_kfold_models and PART_3\Part3_kfold_models to see the results.

g) Bias detection and bias mitigation
1. First for bias detection on the model from Part II, make sure it is downloaded and placed in the models folder.
2. Run run_model.py to generate the results_test_run.csv used for bias detection.
3. Run bias_analysis.py. The metrics across the different demographic groups will be printed and available in bias_analysis_results.csv. Take note of the biases detected (in our case, the old group had poorer accuracy).
4. Bias mitigation can be done by manually adding images of old individuals in data/biasMitigationExtraImageBatch, and then placing the images accordingly to their attributes (gender, age). Note: Images must be manually selected from the original FER-2013/LFIW/KDEF datasets used in this project as they are already labelled for facial expression.
5. Once sufficient images have been added, run create_bias_mitigation_dataset.py to update the large CSV linking all images to their labels correctly. Then, run unbiased_model_training.py to generate a new model based on the new dataset. Copy the name.
6. Paste the name of the new model in run_model.py to generate new predictions on the test dataset in results_test_run.csv.
7. Run bias_analysis.py and notice the improvements (or not...) in terms of bias with the new model.
8. The new model can also be evaluated with unbiased_model_evaluation.py for standard metrics.
